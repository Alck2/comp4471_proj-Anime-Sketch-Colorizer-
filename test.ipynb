{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BhJjrDThK6Zl"
   },
   "source": [
    "# **1. Basics**\n",
    "**Prerequisites**\n",
    "\n",
    "`pytorch`, `torchvision`, `numpy`, `openCV2`,`matplotlib`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1659,
     "status": "ok",
     "timestamp": 1573317495244,
     "user": {
      "displayName": "친절배려양보",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDFkwnEU7dwPLhibVEPf81KRoxg6Fp6WyoycQ8S_w=s64",
      "userId": "13918714700772710322"
     },
     "user_tz": -540
    },
    "id": "I3KEZd2bd1Pm",
    "outputId": "ec9a9597-26e1-43aa-aa11-af810d9725fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The gpu to be used : NVIDIA GeForce MX250\n"
     ]
    }
   ],
   "source": [
    "# For plotting\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# For utilities\n",
    "import time, sys, os\n",
    "\n",
    "# For conversion\n",
    "import cv2\n",
    "import opencv_transforms.transforms as TF\n",
    "import dataloader\n",
    "\n",
    "# For everything\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "# For our model\n",
    "import mymodels\n",
    "import torchvision.models\n",
    "\n",
    "# To ignore warning\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "if device=='cuda':\n",
    "    print(\"The gpu to be used : {}\".format(torch.cuda.get_device_name(0)))\n",
    "else:\n",
    "    print(\"No gpu detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3_cw23W9d1P5"
   },
   "source": [
    "# **2. Loading dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WfPjBNrcAXLw"
   },
   "source": [
    "## 2.1 Color to sketch converter\n",
    "\n",
    "The `netC2S` is a network that convert a colorful image to sketch image. The network was pretrained on the dataset [1]. Even though there are some other methods to convert image to sketch like edge detection, Just forwding another pretrained network gives better result.\n",
    "\n",
    "To load the model weights, download the checkpoint on https://drive.google.com/open?id=1pIZCjubtyOUr7AXtGQMvzcbKczJ9CtQG (449MB) and unzip on directory `./checkpoint`. \n",
    "Then the file on `./checkpoint/color2edge/ckpt.pth` will be loaded.\n",
    "\n",
    "[1] Taebum Kim, \"Anime Sketch Colorization Pair\", https://www.kaggle.com/ktaebum/anime-sketch-colorization-pair, 2019., 2020.1.13."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained Color2Sketch model... Done!\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    netC2S = mymodels.Color2Sketch(pretrained=True).to(device)\n",
    "    netC2S.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W0wM8z9abn-P"
   },
   "source": [
    "## 2.2 Load data\n",
    "\n",
    "To pre-process input images, the module `opencv_transforms.transforms` and `opencv_transforms.functional` are used. These are implemented with **openCV** so much faster than `torchvision.transforms` which is based on **Pillow**.[2] You can download the module on [2].\n",
    "\n",
    "To download validation dataset, go [1] and download. Unzip the images on directory `./dataset/val/`.\n",
    "\n",
    "I've alreay set some validation and test images. If you want test on orther images, put the image on `./dataset/test/`. \n",
    "\n",
    "[2] Jim Bohnslav,\"opencv_transforms\", https://github.com/jbohnslav/opencv_transforms, 2020.1.13."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 92
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1035,
     "status": "ok",
     "timestamp": 1573317630904,
     "user": {
      "displayName": "친절배려양보",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDFkwnEU7dwPLhibVEPf81KRoxg6Fp6WyoycQ8S_w=s64",
      "userId": "13918714700772710322"
     },
     "user_tz": -540
    },
    "id": "mQe2xM2sAbVy",
    "outputId": "dc0237b9-6c86-430e-d239-214be395419b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Validation data... Done!\n",
      "Validation data size : 6002\n",
      "Loading Test data... Done!\n",
      "Test data size : 1434\n",
      "Loading Reference data... "
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] 系統找不到指定的路徑。: './dataset/reference'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 31>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoading Reference data...\u001b[39m\u001b[38;5;124m'\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     28\u001b[0m refer_transforms \u001b[38;5;241m=\u001b[39m TF\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[0;32m     29\u001b[0m     TF\u001b[38;5;241m.\u001b[39mResize(\u001b[38;5;241m512\u001b[39m),\n\u001b[0;32m     30\u001b[0m     ])\n\u001b[1;32m---> 31\u001b[0m refer_imagefolder \u001b[38;5;241m=\u001b[39m \u001b[43mdataloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGetImageFolder\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./dataset/reference\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrefer_transforms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnetC2S\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mncluster\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m refer_loader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(refer_imagefolder, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     33\u001b[0m refer_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(refer_loader))\n",
      "File \u001b[1;32m~\\Desktop\\HKUST\\comp4471\\project\\comp4471_proj-Anime-Sketch-Colorizer-\\dataloader.py:146\u001b[0m, in \u001b[0;36mGetImageFolder.__init__\u001b[1;34m(self, root, transform, sketch_net, ncluster)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, root, transform, sketch_net, ncluster):\n\u001b[1;32m--> 146\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mGetImageFolder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    147\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mncluster \u001b[38;5;241m=\u001b[39m ncluster\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msketch_net \u001b[38;5;241m=\u001b[39m sketch_net\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\comp4471_proj\\lib\\site-packages\\torchvision\\datasets\\folder.py:310\u001b[0m, in \u001b[0;36mImageFolder.__init__\u001b[1;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    303\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    304\u001b[0m     root: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    308\u001b[0m     is_valid_file: Optional[Callable[[\u001b[38;5;28mstr\u001b[39m], \u001b[38;5;28mbool\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m ):\n\u001b[1;32m--> 310\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mIMG_EXTENSIONS\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_transform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_transform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_valid_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\comp4471_proj\\lib\\site-packages\\torchvision\\datasets\\folder.py:145\u001b[0m, in \u001b[0;36mDatasetFolder.__init__\u001b[1;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    137\u001b[0m     root: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    142\u001b[0m     is_valid_file: Optional[Callable[[\u001b[38;5;28mstr\u001b[39m], \u001b[38;5;28mbool\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    143\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(root, transform\u001b[38;5;241m=\u001b[39mtransform, target_transform\u001b[38;5;241m=\u001b[39mtarget_transform)\n\u001b[1;32m--> 145\u001b[0m     classes, class_to_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_classes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    146\u001b[0m     samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_dataset(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot, class_to_idx, extensions, is_valid_file)\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader \u001b[38;5;241m=\u001b[39m loader\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\comp4471_proj\\lib\\site-packages\\torchvision\\datasets\\folder.py:219\u001b[0m, in \u001b[0;36mDatasetFolder.find_classes\u001b[1;34m(self, directory)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_classes\u001b[39m(\u001b[38;5;28mself\u001b[39m, directory: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[List[\u001b[38;5;28mstr\u001b[39m], Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]]:\n\u001b[0;32m    193\u001b[0m     \u001b[38;5;124;03m\"\"\"Find the class folders in a dataset structured as follows::\u001b[39;00m\n\u001b[0;32m    194\u001b[0m \n\u001b[0;32m    195\u001b[0m \u001b[38;5;124;03m        directory/\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;124;03m        (Tuple[List[str], Dict[str, int]]): List of all classes and dictionary mapping each class to an index.\u001b[39;00m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind_classes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\comp4471_proj\\lib\\site-packages\\torchvision\\datasets\\folder.py:41\u001b[0m, in \u001b[0;36mfind_classes\u001b[1;34m(directory)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_classes\u001b[39m(directory: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[List[\u001b[38;5;28mstr\u001b[39m], Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]]:\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;124;03m\"\"\"Finds the class folders in a dataset.\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \n\u001b[0;32m     39\u001b[0m \u001b[38;5;124;03m    See :class:`DatasetFolder` for details.\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m     classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(entry\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscandir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m entry\u001b[38;5;241m.\u001b[39mis_dir())\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m classes:\n\u001b[0;32m     43\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find any class folder in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdirectory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] 系統找不到指定的路徑。: './dataset/reference'"
     ]
    }
   ],
   "source": [
    "# batch_size. number of cluster\n",
    "batch_size = 1\n",
    "ncluster = 9\n",
    "\n",
    "# Validation \n",
    "print('Loading Validation data...', end=' ')\n",
    "val_transforms = TF.Compose([\n",
    "    TF.Resize(512),\n",
    "    ])\n",
    "val_imagefolder = dataloader.PairImageFolder('./dataset/val', val_transforms, netC2S, ncluster)\n",
    "val_loader = torch.utils.data.DataLoader(val_imagefolder, batch_size=batch_size, shuffle=False)\n",
    "print(\"Done!\")\n",
    "print(\"Validation data size : {}\".format(len(val_imagefolder)))\n",
    "\n",
    "\n",
    "# Test\n",
    "print('Loading Test data...', end=' ')\n",
    "test_transforms = TF.Compose([\n",
    "    TF.Resize(512),\n",
    "    ])\n",
    "test_imagefolder = dataloader.GetImageFolder('./dataset/test', test_transforms, netC2S, ncluster)\n",
    "test_loader = torch.utils.data.DataLoader(test_imagefolder, batch_size=batch_size, shuffle=False)\n",
    "print(\"Done!\")\n",
    "print(\"Test data size : {}\".format(len(test_imagefolder)))\n",
    "\n",
    "# Reference\n",
    "print('Loading Reference data...', end=' ')\n",
    "refer_transforms = TF.Compose([\n",
    "    TF.Resize(512),\n",
    "    ])\n",
    "refer_imagefolder = dataloader.GetImageFolder('./dataset/reference', refer_transforms, netC2S, ncluster)\n",
    "refer_loader = torch.utils.data.DataLoader(refer_imagefolder, batch_size=1, shuffle=False)\n",
    "refer_batch = next(iter(refer_loader))\n",
    "print(\"Done!\")\n",
    "print(\"Reference data size : {}\".format(len(refer_imagefolder)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DcBgaktjD3Ul"
   },
   "source": [
    "## 2.3 Dataset Test\n",
    "\n",
    "Check the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_batch_iter = iter(refer_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 216
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1797,
     "status": "ok",
     "timestamp": 1573317635511,
     "user": {
      "displayName": "친절배려양보",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDFkwnEU7dwPLhibVEPf81KRoxg6Fp6WyoycQ8S_w=s64",
      "userId": "13918714700772710322"
     },
     "user_tz": -540
    },
    "id": "90y9V_zqeb93",
    "outputId": "aad3aaf3-ffcc-48ed-ccca-da0c48adc607",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "temp_batch = next(temp_batch_iter)\n",
    "edge = temp_batch[0]\n",
    "color = temp_batch[1]\n",
    "color_palette = temp_batch[2]\n",
    "\n",
    "plt.figure(figsize=(16, 100))\n",
    "result =torch.cat([edge, color]+color_palette,dim=-2)\n",
    "plt.imshow(np.transpose(vutils.make_grid(result, nrow=6, padding=5, normalize=True).cpu(),(1,2,0)))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Images\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cdYb-1jfE8Lh"
   },
   "source": [
    "# **3. Load the Model**\n",
    "\n",
    "The model are implemented on `mymodels.py`.\n",
    "\n",
    "To load the model weights, download the checkpoint on https://drive.google.com/open?id=1pIZCjubtyOUr7AXtGQMvzcbKczJ9CtQG (449MB) and unzip on directory `./checkpoint`. \n",
    "Then the file on `./checkpoint/edge2color/ckpt.pth` will be loaded.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A : Edge, B : Color\n",
    "nc = 3 * (ncluster + 1)\n",
    "netG = mymodels.Sketch2Color(nc=nc, pretrained=True).to(device) \n",
    "\n",
    "num_params = sum(p.numel() for p in netG.parameters() if p.requires_grad)\n",
    "print('Number of parameters: %d' % (num_params))\n",
    "\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tFSZEPYJd1Q7"
   },
   "source": [
    "# **4. Results**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Show colorization results\n",
    "\n",
    "Show colorization results on val/test batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_batch_iter=iter(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "netG.eval()\n",
    "temp_batch = next(temp_batch_iter)\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    edge = temp_batch[0].to(device)\n",
    "    real = temp_batch[1].to(device)\n",
    "    reference = refer_batch[1].to(device)\n",
    "    color_palette = refer_batch[2]\n",
    "    input_tensor = torch.cat([edge.cpu()]+color_palette, dim=1).to(device)\n",
    "    fake = netG(input_tensor)\n",
    "    result = torch.cat((reference, edge, fake), dim=-1).cpu()\n",
    "    \n",
    "    plt.figure(figsize=(64,64))    \n",
    "    plt.imshow(vutils.make_grid(result, nrow=1, padding=5, normalize=True).cpu().permute(1,2,0))    \n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Reference/Edge/Fake/Real\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Save colorization results\n",
    "\n",
    "Save colorization results on val/test batch on `./outputs/sketch2color` directory.\n",
    "\n",
    "The saved images have form of **Reference/Edge/Colorization Result/Ground Truth**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation funciton\n",
    "def save_images(data_loader, model):\n",
    "    with torch.no_grad():\n",
    "        # Prepare value counters and timers\n",
    "        for i, data in enumerate(data_loader, 0):\n",
    "            # Record time to do forward passes and save images\n",
    "            start = time.time()\n",
    "            \n",
    "            edge = data[0].to(device)\n",
    "            edge = torch.cat([edge]*6, dim=0)\n",
    "            real = data[1].to(device)\n",
    "            real = torch.cat([real]*6, dim=0)\n",
    "            reference = refer_batch[1].to(device)\n",
    "            color_palette = refer_batch[2]\n",
    "            input_tensor = torch.cat([edge.cpu()]+color_palette, dim=1).to(device)\n",
    "            fake = netG(input_tensor)\n",
    "            result = torch.cat((reference, edge, fake, real), dim=-1)\n",
    "            output = vutils.make_grid(result, nrow=1, padding=5, normalize=True).cpu().permute(1,2,0).numpy()\n",
    "            \n",
    "            # Save images to file\n",
    "            save_path = 'outputs/sketch2color/'\n",
    "            save_name = 'img-{}.jpg'.format(i)\n",
    "            plt.imsave(arr=output, fname='{}{}'.format(save_path, save_name))\n",
    "\n",
    "            # Record time to do forward passes and save images\n",
    "            end = time.time()\n",
    "\n",
    "            # Print model accuracy -- in the code below, val refers to both value and validation\n",
    "            print('\\rSave: [{0}/{1}]\\t'\n",
    "                'Time {2:0.3f}s \\t'\n",
    "                .format(i+1, len(data_loader), end - start), end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    netG.eval()\n",
    "    save_images(val_loader, netG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "main.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
